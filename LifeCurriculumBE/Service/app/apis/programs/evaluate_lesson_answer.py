# from datetime import datetime
# from typing import Optional
# from fastapi import APIRouter, HTTPException
# from pydantic import BaseModel, Field, model_validator

# from app.storage.program_store import get_lesson_by_uuid
# from app.models.lesson import Lesson
# from app.models.openai_models import TextGenerationRequest
# from app.daos.openai_dao import OpenAIDAO
# from app.logger import get_logger

# logger = get_logger(__name__)
# router = APIRouter(prefix="/programs", tags=["Programs"])
# dao = OpenAIDAO()

# # =========================
# # Request / Response Models
# # =========================

# class EvaluateAnswerRequest(BaseModel):
#     """
#     Unified request supporting all interaction types:
#     - reflection_prompt / teach_back / self_explanation -> user_answer (str) OR field_values (dict[str,str])
#     - multiple_choice                                   -> selected_option (str)
#     - ordering_interaction                              -> selected_order (list[str])
#     - matching_interaction                              -> selected_pairs (dict[str,str])
#     """
#     user_answer: Optional[str] = Field(None, max_length=4000)
#     field_values: Optional[dict[str, str]] = Field(
#         default=None, description="Keyed by config.fields[].id for typed interactions"
#     )
#     selected_option: Optional[str] = None
#     selected_order: Optional[list[str]] = None
#     selected_pairs: Optional[dict[str, str]] = None
#     slider_value: Optional[int] = None  # legacy

#     @model_validator(mode="after")
#     def ensure_some_input(self):
#         if (self.user_answer and self.user_answer.strip()): return self
#         if self.field_values: return self
#         if self.selected_option: return self
#         if self.selected_order: return self
#         if self.selected_pairs: return self
#         if self.slider_value is not None: return self
#         raise ValueError("Provide one of: user_answer, field_values, selected_option, selected_order, selected_pairs, slider_value.")

# class EvaluateAnswerResponse(BaseModel):
#     feedback: str = Field(..., description="Educational feedback generated by OpenAI")
#     lesson_title: str = Field(..., description="Title of the lesson for context")
#     prompt_text: str = Field(..., description="The original prompt that was answered")
#     evaluation_timestamp: datetime = Field(
#         default_factory=datetime.now, description="When the evaluation was performed"
#     )
#     model_used: Optional[str] = Field(default=None, description="OpenAI model used")

# # =========================
# # Prompt Builder
# # =========================

# # def build_evaluation_prompt(lesson: Lesson, body: EvaluateAnswerRequest, *, is_correct: Optional[bool] = None) -> str:
# #     """
# #     Build a context-aware evaluation prompt tailored to the interaction type.
# #     For multiple_choice, pass is_correct so the LLM can tailor tone/explanation.
# #     """
# #     # Conversation context (for grounding)
# #     conversation_context = ""
# #     if lesson.conversation_chunks:
# #         lines = [f"{chunk.speaker}: {chunk.text}" for chunk in lesson.conversation_chunks]
# #         conversation_context = "\n".join(lines)

# #     # Primary interaction prompt (what the learner is responding to)
# #     interaction_prompt = ""
# #     interaction_type = ""
# #     if lesson.primary_interaction:
# #         interaction_prompt = lesson.primary_interaction.prompt or ""
# #         interaction_type = (lesson.primary_interaction.type or "").strip()

# #     # Learner response (normalize by type)
# #     if interaction_type == "reflection_prompt":
# #         learner_response = (body.user_answer or "").strip()
# #     elif interaction_type == "slider_scale":
# #         learner_response = f"Slider response: {body.slider_value}"
# #     elif interaction_type == "multiple_choice":
# #         learner_response = f"Selected option: {body.selected_option}"
# #     else:
# #         learner_response = (body.user_answer or "").strip()

# #     # Style guidance by type
# #     if interaction_type == "reflection_prompt":
# #         style_block = (
# #             "• Offer supportive feedback in a warm, conversational tone.\n"
# #             "• Reflect back one strength in their reflection and one area to deepen.\n"
# #             "• Suggest a gentle next step or question that connects to the lesson.\n"
# #             "• Exactly 2 short paragraphs (3–4 sentences each), 120–180 words."
# #         )
# #     elif interaction_type == "slider_scale":
# #         style_block = (
# #             "• Acknowledge their rating and what it might imply.\n"
# #             "• Offer one concrete idea to nudge the score a point higher next time.\n"
# #             "• Keep it encouraging and specific to the conversation’s mechanisms.\n"
# #             "• Exactly 2 short paragraphs (3–4 sentences each), 90–140 words."
# #         )
# #     elif interaction_type == "multiple_choice":
# #         verdict = "correct" if is_correct else "incorrect"
# #         style_block = (
# #             f"• The learner's answer was {verdict}. Start by acknowledging that succinctly.\n"
# #             "• Briefly explain the reasoning using a concrete detail from the conversation.\n"
# #             "• If incorrect, add a quick tip to remember the correct idea next time.\n"
# #             "• Keep it crisp and friendly, 70–120 words total."
# #         )
# #     else:
# #         style_block = (
# #             "• Warm, encouraging, and practical.\n"
# #             "• Reflect a key strength and offer one next step.\n"
# #             "• Exactly 2 short paragraphs (3–4 sentences each), 120–180 words."
# #         )

# #     # For MCQ, include options and correct option explicitly
# #     options_block = ""
# #     if interaction_type == "multiple_choice":
# #         opts = getattr(lesson.primary_interaction, "options", None) or []
# #         correct = getattr(lesson.primary_interaction, "correct_option", None) or ""
# #         options_text = "\n".join(f"- {o}" for o in opts)
# #         options_block = f"\nOPTIONS\n{options_text}\n\nCORRECT OPTION\n{correct}\n"

# #     prompt = f"""You are a thoughtful mentor responding to a learner.

# # PURPOSE
# # - Provide supportive, human feedback that feels like a continuation of the lesson.
# # - Connect your feedback to the lesson’s discussion and the user’s response.
# # - Offer one clear suggestion or question they can apply next.

# # STYLE
# # {style_block}

# # LESSON
# # Title: {lesson.title}
# # Description: {lesson.description}
# # Listening Context: {lesson.context}

# # CONVERSATION (for grounding)
# # {conversation_context}
# # {options_block}
# # INTERACTION PROMPT
# # {interaction_prompt}

# # LEARNER RESPONSE
# # {learner_response}

# # TASK
# # Write feedback following the STYLE block above. Plain text only."""
# #     return prompt

# def build_evaluation_prompt(lesson: Lesson, body: EvaluateAnswerRequest, *, is_correct: Optional[bool] = None) -> str:
#     # Conversation context (for grounding)
#     conversation_context = ""
#     if lesson.conversation_chunks:
#         lines = [f"{chunk.speaker}: {chunk.text}" for chunk in lesson.conversation_chunks]
#         conversation_context = "\n".join(lines)

#     # Primary interaction
#     interaction_prompt = lesson.primary_interaction.prompt or "" if lesson.primary_interaction else ""
#     interaction_type = (lesson.primary_interaction.type or "").strip() if lesson.primary_interaction else ""

#     # Normalize learner response for the prompt (string view for the LLM)
#     def pretty_fields(d: Optional[dict[str, str]]) -> str:
#         if not d: return ""
#         items = [f"- {k}: {v}" for k, v in d.items()]
#         return "\n".join(items)

#     if interaction_type in {"reflection_prompt", "teach_back", "self_explanation"}:
#         learner_response = (body.user_answer or "").strip()
#         # If structured fields provided, append a readable view
#         if body.field_values:
#             learner_response = (learner_response + "\n\nFIELDS\n" + pretty_fields(body.field_values)).strip()
#     elif interaction_type == "multiple_choice":
#         learner_response = f"Selected option: {body.selected_option}"
#     elif interaction_type == "ordering_interaction":
#         learner_response = "Selected order:\n" + "\n".join(f"{i+1}. {v}" for i, v in enumerate(body.selected_order or []))
#     elif interaction_type == "matching_interaction":
#         pairs_str = "\n".join(f"- {k} → {v}" for k, v in (body.selected_pairs or {}).items())
#         learner_response = f"Selected pairs:\n{pairs_str}"
#     # elif interaction_type == "slider_scale":
#     #     learner_response = f"Slider response: {body.slider_value}"
#     else:
#         learner_response = (body.user_answer or "").strip()

#     # Style guidance by type
#     if interaction_type == "reflection_prompt":
#         style_block = (
#             "• Warm, conversational feedback.\n"
#             "• Reflect one strength and one way to deepen.\n"
#             "• Suggest a next step tied to the conversation.\n"
#             "• 2 short paragraphs (3–4 sentences each), 120–180 words."
#         )
#     elif interaction_type == "teach_back":
#         style_block = (
#             "• Start by acknowledging clarity/utility of their explanation.\n"
#             "• If a misconception field was included, check it and correct or strengthen it.\n"
#             "• Offer one improvement to make it actionable for a beginner.\n"
#             "• ≈120–180 words, crisp and concrete."
#         )
#     elif interaction_type == "self_explanation":
#         style_block = (
#             "• Give stepwise feedback: note one step that’s well-justified and one that needs clearer reasoning.\n"
#             "• Tie back to the example in the conversation.\n"
#             "• End with a concise rule-of-thumb.\n"
#             "• ≈120–180 words."
#         )
#     elif interaction_type == "multiple_choice":
#         if is_correct is None:
#             style_block = (
#                 "• Acknowledge their choice and explain the reasoning behind it.\n"
#                 "• Use a concrete detail from the conversation to support your explanation.\n"
#                 "• Offer a practical insight they can apply next time.\n"
#                 "• 70–120 words."
#             )
#         else:
#             verdict = "correct" if is_correct else "incorrect"
#             style_block = (
#                 f"• The learner's answer was {verdict}. Acknowledge that succinctly.\n"
#                 "• Explain the reasoning using a concrete detail from the conversation.\n"
#                 "• If incorrect, add a quick tip to remember the idea next time.\n"
#                 "• 70–120 words."
#             )
#     elif interaction_type == "ordering_interaction":
#         if is_correct is None:
#             style_block = (
#                 "• Acknowledge their sequence and explain why order matters for this topic.\n"
#                 "• Reference the dialogue's phrasing that suggests the flow of ideas.\n"
#                 "• Offer a useful framework for remembering key sequences.\n"
#                 "• 70–120 words."
#             )
#         else:
#             verdict = "correct" if is_correct else "incorrect"
#             style_block = (
#                 f"• Their sequence is {verdict}. State the key reason the correct order matters.\n"
#                 "• Reference the dialogue's phrasing that implies the correct order.\n"
#                 "• Add a quick mnemonic for remembering the order.\n"
#                 "• 70–120 words."
#             )
#     elif interaction_type == "matching_interaction":
#         verdict = "correct" if is_correct else "partially correct" if is_correct is None else "incorrect"
#         style_block = (
#             f"• The matches are {verdict}. Highlight one especially important match and why it fits.\n"
#             "• Gently fix any mismatches with a brief cue.\n"
#             "• 70–120 words."
#         )
#     # elif interaction_type == "slider_scale":
#     #     style_block = (
#     #         "• Acknowledge their rating and what it implies.\n"
#     #         "• Offer one concrete idea to nudge the score a point higher next time.\n"
#     #         "• 90–140 words."
#     #     )
#     else:
#         style_block = (
#             "• Warm, encouraging, and practical.\n"
#             "• Reflect a key strength and offer one next step.\n"
#             "• 120–180 words."
#         )

#     # Include options/correctness context for discrete tasks
#     options_block = ""
#     if interaction_type == "multiple_choice":
#         opts = getattr(lesson.primary_interaction, "options", None) or []
#         correct = getattr(lesson.primary_interaction, "correct_option", None) or ""
#         options_text = "\n".join(f"- {o}" for o in opts)
#         options_block = f"\nOPTIONS\n{options_text}\n\nCORRECT OPTION\n{correct}\n"
#     elif interaction_type == "ordering_interaction":
#         opts = getattr(lesson.primary_interaction, "options", None) or []
#         corr = getattr(lesson.primary_interaction, "correct_order", None) or []
#         options_text = "\n".join(f"- {o}" for o in opts)
#         corr_text = "\n".join(f"{i+1}. {v}" for i, v in enumerate(corr))
#         options_block = f"\nITEMS\n{options_text}\n\nCORRECT ORDER\n{corr_text}\n"
#     elif interaction_type == "matching_interaction":
#         pairs = getattr(lesson.primary_interaction, "pairs", None) or {}
#         pairs_text = "\n".join(f"- {k} → {v}" for k, v in pairs.items())
#         options_block = f"\nCORRECT PAIRS\n{pairs_text}\n"

#     prompt = f"""You are a thoughtful mentor responding to a learner.

# PURPOSE
# - Provide supportive, human feedback that feels like a continuation of the lesson.
# - Connect your feedback to the lesson’s discussion and the user’s response.
# - Offer one clear suggestion or rule-of-thumb they can apply next.

# STYLE
# {style_block}

# LESSON
# Title: {lesson.title}
# Description: {lesson.description}
# Listening Context: {lesson.context}

# CONVERSATION (for grounding)
# {conversation_context}
# {options_block}
# INTERACTION PROMPT
# {interaction_prompt}

# LEARNER RESPONSE
# {learner_response}

# TASK
# Write feedback following the STYLE block above. Plain text only."""
#     return prompt

# # =========================
# # Route
# # =========================

# @router.post(
#     "/lessons/{lesson_id}/evaluate-answer",
#     response_model=EvaluateAnswerResponse,
#     summary="Evaluate Lesson Answer",
#     description="Evaluate a user's response (reflection/slider/multiple_choice) and return gentle, educational feedback."
# )
# async def evaluate_lesson_answer(lesson_id: str, body: EvaluateAnswerRequest):
#     logger.info(f"Evaluating answer for lesson {lesson_id}")

#     # 1) Fetch lesson
#     result = get_lesson_by_uuid(lesson_id)
#     if not result:
#         logger.warning(f"Lesson {lesson_id} not found")
#         raise HTTPException(status_code=404, detail="Lesson not found")

#     program_dict, lesson_dict = result

#     # 2) Parse lesson
#     try:
#         lesson = Lesson(**lesson_dict)
#     except Exception as e:
#         logger.error(f"Failed to parse lesson {lesson_id}: {e}")
#         raise HTTPException(status_code=500, detail="Invalid lesson data")

#     # 3) Validate primary interaction exists
#     if not lesson.primary_interaction:
#         logger.warning(f"Lesson {lesson_id} has no primary interaction")
#         raise HTTPException(status_code=400, detail="Lesson has no primary interaction to evaluate")

#     # 4) Type-aware validation + correctness
#     pi = lesson.primary_interaction
#     itype = (pi.type or "").strip()

#     is_correct: Optional[bool] = None

#     if itype in {"reflection_prompt", "teach_back", "self_explanation"}:
#         # accept either free text or structured fields
#         has_text = bool(body.user_answer and body.user_answer.strip())
#         has_fields = bool(body.field_values)
#         if not (has_text or has_fields):
#             raise HTTPException(status_code=422, detail=f"This prompt ({itype}) requires 'user_answer' or 'field_values'.")
#         # Optional: enforce max_chars from config.fields
#         cfg = getattr(pi, "config", None)
#         if cfg and getattr(cfg, "fields", None) and body.field_values:
#             for fld in cfg.fields:
#                 fid, maxc = getattr(fld, "id", None), getattr(fld, "max_chars", None)
#                 if fid and maxc and fid in body.field_values:
#                     if len(body.field_values[fid]) > maxc:
#                         raise HTTPException(status_code=422, detail=f"Field '{fid}' exceeds max_chars={maxc}.")
#     elif itype == "multiple_choice":
#         options = getattr(pi, "options", None) or []
#         correct = getattr(pi, "correct_option", None)
        
#         # Handle missing or invalid lesson configuration gracefully
#         if not options or not isinstance(options, list):
#             logger.warning(f"Multiple choice lesson {lesson_id} missing or invalid options, treating as text response")
#             is_correct = None
#         elif not correct:
#             logger.warning(f"Multiple choice lesson {lesson_id} missing correct_option, cannot determine correctness")
#             # Still validate user provided a valid option
#             if not (body.selected_option and body.selected_option in options):
#                 raise HTTPException(status_code=422, detail="Provide 'selected_option' that matches one of the available options.")
#             is_correct = None  # Cannot determine correctness without correct_option
#         else:
#             # Normal case: validate user input
#             if not (body.selected_option and body.selected_option in options):
#                 raise HTTPException(status_code=422, detail="Provide 'selected_option' that matches one of the options.")
#             is_correct = (body.selected_option == correct)
#     elif itype == "ordering_interaction":
#         opts = getattr(pi, "options", None) or []
#         corr = getattr(pi, "correct_order", None) or []
#         sel = body.selected_order or []
        
#         # Handle missing or invalid lesson configuration gracefully
#         if not opts:
#             # If no options, treat as a generic text response and continue
#             logger.warning(f"Ordering interaction lesson {lesson_id} missing options, treating as text response")
#             is_correct = None
#         elif not corr:
#             # If missing correct_order, assume the current options order is correct as fallback
#             logger.warning(f"Ordering interaction lesson {lesson_id} missing correct_order, using options order as fallback")
#             corr = opts.copy()
#             # Validate user input
#             if not sel or set(sel) != set(opts) or len(sel) != len(opts):
#                 raise HTTPException(status_code=422, detail="selected_order must be a permutation of the available options.")
#             is_correct = (sel == corr)
#         elif set(opts) != set(corr) or len(opts) != len(corr):
#             # If options and correct_order don't match, use options as the fallback
#             logger.warning(f"Ordering interaction lesson {lesson_id} has mismatched options and correct_order, using options as fallback")
#             corr = opts.copy()
#             # Validate user input
#             if not sel or set(sel) != set(opts) or len(sel) != len(opts):
#                 raise HTTPException(status_code=422, detail="selected_order must be a permutation of the available options.")
#             is_correct = (sel == corr)
#         else:
#             # Normal case: validate user input
#             if not sel or set(sel) != set(opts) or len(sel) != len(opts):
#                 raise HTTPException(status_code=422, detail="selected_order must be a permutation of options.")
#             is_correct = (sel == corr)
#     elif itype == "matching_interaction":
#         pairs = getattr(pi, "pairs", None) or {}
#         sel = body.selected_pairs or {}
        
#         # Handle missing or invalid lesson configuration gracefully
#         if not isinstance(pairs, dict) or not pairs:
#             logger.warning(f"Matching interaction lesson {lesson_id} missing or invalid pairs, treating as text response")
#             is_correct = None
#         else:
#             # Validate user input
#             if not isinstance(sel, dict) or set(sel.keys()) != set(pairs.keys()):
#                 raise HTTPException(status_code=422, detail="selected_pairs must provide a value for each concept key.")
#             # exact-match correctness
#             is_correct = all(pairs[k] == sel.get(k) for k in pairs.keys())
#     # elif itype == "slider_scale":
#     #     if body.slider_value is None:
#     #         raise HTTPException(status_code=422, detail="This slider requires 'slider_value'.")
#     #     try:
#     #         smin = int(getattr(pi, "scale_min", 1) or 1)
#     #         smax = int(getattr(pi, "scale_max", 10) or 10)
#     #     except Exception:
#     #         smin, smax = 1, 10
#     #     if not (smin <= body.slider_value <= smax):
#     #         raise HTTPException(status_code=422, detail=f"slider_value must be between {smin} and {smax}.")
#     else:
#         raise HTTPException(status_code=400, detail=f"Unsupported interaction type '{itype}'.")

#     # 5) Build prompt tailored to interaction type
#     evaluation_prompt = build_evaluation_prompt(lesson, body, is_correct=is_correct)

#     # 6) Call LLM
#     try:
#         request = TextGenerationRequest(messages=[{"role": "user", "content": evaluation_prompt}])
#         response = await dao.generate_text(request)
#         feedback = (response.text or "").strip()
#         model_used = getattr(response, "model", None)
#         logger.info(f"Generated feedback for lesson {lesson_id} using model {model_used}")
#     except Exception as e:
#         logger.error(f"OpenAI evaluation failed for lesson {lesson_id}: {e}")
#         raise HTTPException(status_code=500, detail="Failed to generate evaluation feedback")

#     # 7) Return
#     return EvaluateAnswerResponse(
#         feedback=feedback,
#         lesson_title=lesson.title,
#         prompt_text=lesson.primary_interaction.prompt or "",
#         evaluation_timestamp=datetime.now(),
#         model_used=model_used,
#     )
from datetime import datetime
from typing import Optional
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field, model_validator

from app.storage.program_store import get_lesson_by_uuid
from app.models.lesson import Lesson
from app.models.openai_models import TextGenerationRequest
from app.daos.openai_dao import OpenAIDAO
from app.logger import get_logger

logger = get_logger(__name__)
router = APIRouter(prefix="/programs", tags=["Programs"])
dao = OpenAIDAO()

# =========================
# Request / Response Models
# =========================

class EvaluateAnswerRequest(BaseModel):
    """
    Unified request supporting all interaction types:
    - reflection_prompt / teach_back / self_explanation -> user_answer (str) OR field_values (dict[str,str])
    - multiple_choice                                   -> selected_option (str)
    - ordering_interaction                              -> selected_order (list[str])
    - matching_interaction                              -> selected_pairs (dict[str,str])
    """
    user_answer: Optional[str] = Field(None, max_length=4000)
    field_values: Optional[dict[str, str]] = Field(
        default=None, description="Keyed by config.fields[].id for typed interactions"
    )
    selected_option: Optional[str] = None
    selected_order: Optional[list[str]] = None
    selected_pairs: Optional[dict[str, str]] = None
    slider_value: Optional[int] = None  # legacy

    @model_validator(mode="after")
    def ensure_some_input(self):
        if (self.user_answer and self.user_answer.strip()): return self
        if self.field_values: return self
        if self.selected_option: return self
        if self.selected_order: return self
        if self.selected_pairs: return self
        if self.slider_value is not None: return self
        raise ValueError("Provide one of: user_answer, field_values, selected_option, selected_order, selected_pairs, slider_value.")

class EvaluateAnswerResponse(BaseModel):
    feedback: str = Field(..., description="Educational feedback generated by OpenAI")
    lesson_title: str = Field(..., description="Title of the lesson for context")
    prompt_text: str = Field(..., description="The original prompt that was answered")
    evaluation_timestamp: datetime = Field(
        default_factory=datetime.now, description="When the evaluation was performed"
    )
    model_used: Optional[str] = Field(default=None, description="OpenAI model used")

# =========================
# Prompt Builder
# =========================

# def build_evaluation_prompt(lesson: Lesson, body: EvaluateAnswerRequest, *, is_correct: Optional[bool] = None) -> str:
#     """
#     Build a context-aware evaluation prompt tailored to the interaction type.
#     For multiple_choice, pass is_correct so the LLM can tailor tone/explanation.
#     """
#     # Conversation context (for grounding)
#     conversation_context = ""
#     if lesson.conversation_chunks:
#         lines = [f"{chunk.speaker}: {chunk.text}" for chunk in lesson.conversation_chunks]
#         conversation_context = "\n".join(lines)

#     # Primary interaction prompt (what the learner is responding to)
#     interaction_prompt = ""
#     interaction_type = ""
#     if lesson.primary_interaction:
#         interaction_prompt = lesson.primary_interaction.prompt or ""
#         interaction_type = (lesson.primary_interaction.type or "").strip()

#     # Learner response (normalize by type)
#     if interaction_type == "reflection_prompt":
#         learner_response = (body.user_answer or "").strip()
#     elif interaction_type == "slider_scale":
#         learner_response = f"Slider response: {body.slider_value}"
#     elif interaction_type == "multiple_choice":
#         learner_response = f"Selected option: {body.selected_option}"
#     else:
#         learner_response = (body.user_answer or "").strip()

#     # Style guidance by type
#     if interaction_type == "reflection_prompt":
#         style_block = (
#             "• Offer supportive feedback in a warm, conversational tone.\n"
#             "• Reflect back one strength in their reflection and one area to deepen.\n"
#             "• Suggest a gentle next step or question that connects to the lesson.\n"
#             "• Exactly 2 short paragraphs (3–4 sentences each), 120–180 words."
#         )
#     elif interaction_type == "slider_scale":
#         style_block = (
#             "• Acknowledge their rating and what it might imply.\n"
#             "• Offer one concrete idea to nudge the score a point higher next time.\n"
#             "• Keep it encouraging and specific to the conversation’s mechanisms.\n"
#             "• Exactly 2 short paragraphs (3–4 sentences each), 90–140 words."
#         )
#     elif interaction_type == "multiple_choice":
#         verdict = "correct" if is_correct else "incorrect"
#         style_block = (
#             f"• The learner's answer was {verdict}. Start by acknowledging that succinctly.\n"
#             "• Briefly explain the reasoning using a concrete detail from the conversation.\n"
#             "• If incorrect, add a quick tip to remember the correct idea next time.\n"
#             "• Keep it crisp and friendly, 70–120 words total."
#         )
#     else:
#         style_block = (
#             "• Warm, encouraging, and practical.\n"
#             "• Reflect a key strength and offer one next step.\n"
#             "• Exactly 2 short paragraphs (3–4 sentences each), 120–180 words."
#         )

#     # For MCQ, include options and correct option explicitly
#     options_block = ""
#     if interaction_type == "multiple_choice":
#         opts = getattr(lesson.primary_interaction, "options", None) or []
#         correct = getattr(lesson.primary_interaction, "correct_option", None) or ""
#         options_text = "\n".join(f"- {o}" for o in opts)
#         options_block = f"\nOPTIONS\n{options_text}\n\nCORRECT OPTION\n{correct}\n"

#     prompt = f"""You are a thoughtful mentor responding to a learner.

# PURPOSE
# - Provide supportive, human feedback that feels like a continuation of the lesson.
# - Connect your feedback to the lesson’s discussion and the user’s response.
# - Offer one clear suggestion or question they can apply next.

# STYLE
# {style_block}

# LESSON
# Title: {lesson.title}
# Description: {lesson.description}
# Listening Context: {lesson.context}

# CONVERSATION (for grounding)
# {conversation_context}
# {options_block}
# INTERACTION PROMPT
# {interaction_prompt}

# LEARNER RESPONSE
# {learner_response}

# TASK
# Write feedback following the STYLE block above. Plain text only."""
#     return prompt

def build_evaluation_prompt(lesson: Lesson, body: EvaluateAnswerRequest, *, is_correct: Optional[bool] = None) -> str:
    # Conversation context (for grounding)
    conversation_context = ""
    if lesson.conversation_chunks:
        lines = [f"{chunk.speaker}: {chunk.text}" for chunk in lesson.conversation_chunks]
        conversation_context = "\n".join(lines)

    # Primary interaction
    interaction_prompt = lesson.primary_interaction.prompt or "" if lesson.primary_interaction else ""
    interaction_type = (lesson.primary_interaction.type or "").strip() if lesson.primary_interaction else ""

    # Normalize learner response for the prompt (string view for the LLM)
    def pretty_fields(d: Optional[dict[str, str]]) -> str:
        if not d: return ""
        items = [f"- {k}: {v}" for k, v in d.items()]
        return "\n".join(items)

    if interaction_type in {"reflection_prompt", "teach_back", "self_explanation"}:
        learner_response = (body.user_answer or "").strip()
        # If structured fields provided, append a readable view
        if body.field_values:
            learner_response = (learner_response + "\n\nFIELDS\n" + pretty_fields(body.field_values)).strip()
    elif interaction_type == "multiple_choice":
        learner_response = f"Selected option: {body.selected_option}"
    elif interaction_type == "ordering_interaction":
        learner_response = "Selected order:\n" + "\n".join(f"{i+1}. {v}" for i, v in enumerate(body.selected_order or []))
    elif interaction_type == "matching_interaction":
        pairs_str = "\n".join(f"- {k} → {v}" for k, v in (body.selected_pairs or {}).items())
        learner_response = f"Selected pairs:\n{pairs_str}"
    # elif interaction_type == "slider_scale":
    #     learner_response = f"Slider response: {body.slider_value}"
    else:
        learner_response = (body.user_answer or "").strip()

    # Style guidance by type
    if interaction_type == "reflection_prompt":
        style_block = (
            "• Warm, conversational feedback.\n"
            "• Reflect one strength and one way to deepen.\n"
            "• Suggest a next step tied to the conversation.\n"
            "• 2 short paragraphs (3–4 sentences each), 120–180 words."
        )
    elif interaction_type == "teach_back":
        style_block = (
            "• Start by acknowledging clarity/utility of their explanation.\n"
            "• If a misconception field was included, check it and correct or strengthen it.\n"
            "• Offer one improvement to make it actionable for a beginner.\n"
            "• ≈120–180 words, crisp and concrete."
        )
    elif interaction_type == "self_explanation":
        style_block = (
            "• Give stepwise feedback: note one step that’s well-justified and one that needs clearer reasoning.\n"
            "• Tie back to the example in the conversation.\n"
            "• End with a concise rule-of-thumb.\n"
            "• ≈120–180 words."
        )
    elif interaction_type == "multiple_choice":
        if is_correct is None:
            style_block = (
                "• Acknowledge their choice and explain the reasoning behind it.\n"
                "• Use a concrete detail from the conversation to support your explanation.\n"
                "• Offer a practical insight they can apply next time.\n"
                "• 70–120 words."
            )
        else:
            verdict = "correct" if is_correct else "incorrect"
            style_block = (
                f"• The learner's answer was {verdict}. Acknowledge that succinctly.\n"
                "• Explain the reasoning using a concrete detail from the conversation.\n"
                "• If incorrect, add a quick tip to remember the idea next time.\n"
                "• 70–120 words."
            )
    elif interaction_type == "ordering_interaction":
        if is_correct is None:
            style_block = (
                "• Acknowledge their sequence and explain why order matters for this topic.\n"
                "• Reference the dialogue's phrasing that suggests the flow of ideas.\n"
                "• Offer a useful framework for remembering key sequences.\n"
                "• 70–120 words."
            )
        else:
            verdict = "correct" if is_correct else "incorrect"
            style_block = (
                f"• Their sequence is {verdict}. State the key reason the correct order matters.\n"
                "• Reference the dialogue's phrasing that implies the correct order.\n"
                "• Add a quick mnemonic for remembering the order.\n"
                "• 70–120 words."
            )
    elif interaction_type == "matching_interaction":
        verdict = "correct" if is_correct else "partially correct" if is_correct is None else "incorrect"
        style_block = (
            f"• The matches are {verdict}. Highlight one especially important match and why it fits.\n"
            "• Gently fix any mismatches with a brief cue.\n"
            "• 70–120 words."
        )
    # elif interaction_type == "slider_scale":
    #     style_block = (
    #         "• Acknowledge their rating and what it implies.\n"
    #         "• Offer one concrete idea to nudge the score a point higher next time.\n"
    #         "• 90–140 words."
    #     )
    else:
        style_block = (
            "• Warm, encouraging, and practical.\n"
            "• Reflect a key strength and offer one next step.\n"
            "• 120–180 words."
        )

    # Include options/correctness context for discrete tasks
    options_block = ""
    if interaction_type == "multiple_choice":
        opts = getattr(lesson.primary_interaction, "options", None) or []
        correct = getattr(lesson.primary_interaction, "correct_option", None) or ""
        options_text = "\n".join(f"- {o}" for o in opts)
        options_block = f"\nOPTIONS\n{options_text}\n\nCORRECT OPTION\n{correct}\n"
    elif interaction_type == "ordering_interaction":
        opts = getattr(lesson.primary_interaction, "options", None) or []
        corr = getattr(lesson.primary_interaction, "correct_order", None) or []
        options_text = "\n".join(f"- {o}" for o in opts)
        corr_text = "\n".join(f"{i+1}. {v}" for i, v in enumerate(corr))
        options_block = f"\nITEMS\n{options_text}\n\nCORRECT ORDER\n{corr_text}\n"
    elif interaction_type == "matching_interaction":
        pairs = getattr(lesson.primary_interaction, "pairs", None) or {}
        pairs_text = "\n".join(f"- {k} → {v}" for k, v in pairs.items())
        options_block = f"\nCORRECT PAIRS\n{pairs_text}\n"

    prompt = f"""You are a thoughtful mentor responding to a learner.

PURPOSE
- Provide supportive, human feedback that feels like a continuation of the lesson.
- Connect your feedback to the lesson’s discussion and the user’s response.
- Offer one clear suggestion or rule-of-thumb they can apply next.

STYLE
{style_block}

LESSON
Title: {lesson.title}
Description: {lesson.description}
Listening Context: {lesson.context}

CONVERSATION (for grounding)
{conversation_context}
{options_block}
INTERACTION PROMPT
{interaction_prompt}

LEARNER RESPONSE
{learner_response}

TASK
Write feedback following the STYLE block above. Plain text only."""
    return prompt

# =========================
# Route
# =========================

@router.post(
    "/lessons/{lesson_id}/evaluate-answer",
    response_model=EvaluateAnswerResponse,
    summary="Evaluate Lesson Answer",
    description="Evaluate a user's response (reflection/slider/multiple_choice) and return gentle, educational feedback."
)
async def evaluate_lesson_answer(lesson_id: str, body: EvaluateAnswerRequest):
    logger.info(f"Evaluating answer for lesson {lesson_id}")

    # 1) Fetch lesson
    result = get_lesson_by_uuid(lesson_id)
    if not result:
        logger.warning(f"Lesson {lesson_id} not found")
        raise HTTPException(status_code=404, detail="Lesson not found")

    program_dict, lesson_dict = result

    # 2) Parse lesson
    try:
        lesson = Lesson(**lesson_dict)
    except Exception as e:
        logger.error(f"Failed to parse lesson {lesson_id}: {e}")
        raise HTTPException(status_code=500, detail="Invalid lesson data")

    # 3) Validate primary interaction exists
    if not lesson.primary_interaction:
        logger.warning(f"Lesson {lesson_id} has no primary interaction")
        raise HTTPException(status_code=400, detail="Lesson has no primary interaction to evaluate")

    # 4) Type-aware validation + correctness
    pi = lesson.primary_interaction
    itype = (pi.type or "").strip()

    is_correct: Optional[bool] = None

    if itype in {"reflection_prompt", "teach_back", "self_explanation"}:
        # accept either free text or structured fields
        has_text = bool(body.user_answer and body.user_answer.strip())
        has_fields = bool(body.field_values)
        if not (has_text or has_fields):
            raise HTTPException(status_code=422, detail=f"This prompt ({itype}) requires 'user_answer' or 'field_values'.")
        # Optional: enforce max_chars from config.fields
        cfg = getattr(pi, "config", None)
        if cfg and getattr(cfg, "fields", None) and body.field_values:
            for fld in cfg.fields:
                fid, maxc = getattr(fld, "id", None), getattr(fld, "max_chars", None)
                if fid and maxc and fid in body.field_values:
                    if len(body.field_values[fid]) > maxc:
                        raise HTTPException(status_code=422, detail=f"Field '{fid}' exceeds max_chars={maxc}.")
    elif itype == "multiple_choice":
        options = getattr(pi, "options", None) or []
        correct = getattr(pi, "correct_option", None)
        
        # Handle missing or invalid lesson configuration gracefully
        if not options or not isinstance(options, list):
            logger.warning(f"Multiple choice lesson {lesson_id} missing or invalid options, treating as text response")
            is_correct = None
        elif not correct:
            logger.warning(f"Multiple choice lesson {lesson_id} missing correct_option, cannot determine correctness")
            # Still validate user provided a valid option
            if not (body.selected_option and body.selected_option in options):
                raise HTTPException(status_code=422, detail="Provide 'selected_option' that matches one of the available options.")
            is_correct = None  # Cannot determine correctness without correct_option
        else:
            # Normal case: validate user input
            if not (body.selected_option and body.selected_option in options):
                raise HTTPException(status_code=422, detail="Provide 'selected_option' that matches one of the options.")
            is_correct = (body.selected_option == correct)
    elif itype == "ordering_interaction":
        opts = getattr(pi, "options", None) or []
        corr = getattr(pi, "correct_order", None) or []
        sel = body.selected_order or []
        
        # Handle missing or invalid lesson configuration gracefully
        if not opts:
            # If no options, treat as a generic text response and continue
            logger.warning(f"Ordering interaction lesson {lesson_id} missing options, treating as text response")
            is_correct = None
        elif not corr:
            # If missing correct_order, assume the current options order is correct as fallback
            logger.warning(f"Ordering interaction lesson {lesson_id} missing correct_order, using options order as fallback")
            corr = opts.copy()
            # Validate user input
            if not sel or set(sel) != set(opts) or len(sel) != len(opts):
                raise HTTPException(status_code=422, detail="selected_order must be a permutation of the available options.")
            is_correct = (sel == corr)
        elif set(opts) != set(corr) or len(opts) != len(corr):
            # If options and correct_order don't match, use options as the fallback
            logger.warning(f"Ordering interaction lesson {lesson_id} has mismatched options and correct_order, using options as fallback")
            corr = opts.copy()
            # Validate user input
            if not sel or set(sel) != set(opts) or len(sel) != len(opts):
                raise HTTPException(status_code=422, detail="selected_order must be a permutation of the available options.")
            is_correct = (sel == corr)
        else:
            # Normal case: validate user input
            if not sel or set(sel) != set(opts) or len(sel) != len(opts):
                raise HTTPException(status_code=422, detail="selected_order must be a permutation of options.")
            is_correct = (sel == corr)
    elif itype == "matching_interaction":
        pairs = getattr(pi, "pairs", None) or {}
        sel = body.selected_pairs or {}
        
        # Handle missing or invalid lesson configuration gracefully
        if not isinstance(pairs, dict) or not pairs:
            logger.warning(f"Matching interaction lesson {lesson_id} missing or invalid pairs, treating as text response")
            is_correct = None
        else:
            # Validate user input
            if not isinstance(sel, dict) or set(sel.keys()) != set(pairs.keys()):
                raise HTTPException(status_code=422, detail="selected_pairs must provide a value for each concept key.")
            # exact-match correctness
            is_correct = all(pairs[k] == sel.get(k) for k in pairs.keys())
    # elif itype == "slider_scale":
    #     if body.slider_value is None:
    #         raise HTTPException(status_code=422, detail="This slider requires 'slider_value'.")
    #     try:
    #         smin = int(getattr(pi, "scale_min", 1) or 1)
    #         smax = int(getattr(pi, "scale_max", 10) or 10)
    #     except Exception:
    #         smin, smax = 1, 10
    #     if not (smin <= body.slider_value <= smax):
    #         raise HTTPException(status_code=422, detail=f"slider_value must be between {smin} and {smax}.")
    else:
        raise HTTPException(status_code=400, detail=f"Unsupported interaction type '{itype}'.")

    # 5) Build prompt tailored to interaction type
    evaluation_prompt = build_evaluation_prompt(lesson, body, is_correct=is_correct)

    # 6) Call LLM
    try:
        request = TextGenerationRequest(messages=[{"role": "user", "content": evaluation_prompt}])
        response = await dao.generate_text(request)
        feedback = (response.text or "").strip()
        model_used = getattr(response, "model", None)
        logger.info(f"Generated feedback for lesson {lesson_id} using model {model_used}")
    except Exception as e:
        logger.error(f"OpenAI evaluation failed for lesson {lesson_id}: {e}")
        raise HTTPException(status_code=500, detail="Failed to generate evaluation feedback")

    # 7) Return
    return EvaluateAnswerResponse(
        feedback=feedback,
        lesson_title=lesson.title,
        prompt_text=lesson.primary_interaction.prompt or "",
        evaluation_timestamp=datetime.now(),
        model_used=model_used,
    )
