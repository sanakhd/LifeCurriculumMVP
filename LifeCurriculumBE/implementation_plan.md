# Implementation Plan

Create a new API endpoint that evaluates user answers for lesson prompts and provides educational feedback using OpenAI.

This implementation adds a new evaluation API to the existing LifeCurriculum system that will accept user answers to lesson primary interactions, analyze them using OpenAI's text generation API, and return educational feedback to help users learn and improve. The API will leverage the existing lesson structure, conversation context, and OpenAI integration to provide rich, contextual feedback without persisting the evaluation results.

## [Types]

Define request and response models for the answer evaluation endpoint.

**EvaluateAnswerRequest Model:**
```python
class EvaluateAnswerRequest(BaseModel):
    user_answer: str = Field(..., min_length=1, max_length=2000, description="User's answer to the primary interaction prompt")
    
    @field_validator('user_answer')
    @classmethod
    def validate_non_empty_answer(cls, v: str) -> str:
        if not v.strip():
            raise ValueError('Answer cannot be empty or whitespace only')
        return v.strip()
```

**EvaluateAnswerResponse Model:**
```python
class EvaluateAnswerResponse(BaseModel):
    feedback: str = Field(..., description="Educational feedback generated by OpenAI")
    lesson_title: str = Field(..., description="Title of the lesson for context")
    prompt_text: str = Field(..., description="The original prompt that was answered")
    evaluation_timestamp: datetime = Field(default_factory=datetime.now, description="When the evaluation was performed")
    model_used: Optional[str] = Field(default=None, description="OpenAI model used for evaluation")
```

## [Files]

Create one new endpoint file and update the router to include the new evaluation functionality.

**New Files:**
- `Service/app/apis/programs/evaluate_lesson_answer.py` - Main endpoint implementation with request/response models, prompt building, and evaluation logic

**Modified Files:**
- `Service/app/apis/programs/router.py` - Add import and include the new evaluation router

## [Functions]

Create evaluation endpoint and supporting prompt-building function.

**New Functions:**

`build_evaluation_prompt(lesson: Lesson, user_answer: str) -> str` in `evaluate_lesson_answer.py`:
- Purpose: Construct comprehensive prompt for OpenAI evaluation
- Parameters: Lesson object with conversation and primary interaction, user's answer string
- Returns: Formatted prompt string including conversation context, interaction prompt, user answer, and instructions for educational feedback
- Implementation: Template-based prompt construction emphasizing educational tone and specific lesson context

`evaluate_lesson_answer(lesson_id: str, body: EvaluateAnswerRequest) -> EvaluateAnswerResponse` in `evaluate_lesson_answer.py`:
- Purpose: Main API endpoint for answer evaluation  
- Parameters: lesson_id path parameter, EvaluateAnswerRequest body
- Returns: EvaluateAnswerResponse with feedback and metadata
- Implementation: Lesson retrieval, validation, prompt building, OpenAI API call, response formatting
- Error handling: 404 for missing lesson, 400 for missing primary interaction, 500 for OpenAI errors

## [Classes]

Create Pydantic models for request/response handling.

**New Classes:**

`EvaluateAnswerRequest` in `evaluate_lesson_answer.py`:
- Purpose: Validate and structure incoming answer evaluation requests
- Fields: user_answer (required string with length validation)
- Validation: Non-empty answer validation, whitespace trimming
- Inherits: Pydantic BaseModel

`EvaluateAnswerResponse` in `evaluate_lesson_answer.py`:
- Purpose: Structure the evaluation response with feedback and metadata
- Fields: feedback (required), lesson_title, prompt_text, evaluation_timestamp (auto-generated), model_used (optional)
- Inherits: Pydantic BaseModel

## [Dependencies]

No new external dependencies required.

All required dependencies are already present in the project:
- `fastapi` - API framework (already used)
- `pydantic` - Request/response validation (already used) 
- `openai` - OpenAI API client (already integrated via OpenAIDAO)
- Standard library modules: `datetime`, `typing`

The implementation will use existing project components: OpenAIDAO for AI calls, program_store functions for lesson retrieval, established logging patterns, and existing error handling approaches.

## [Testing]

Test the new evaluation endpoint with various answer types and edge cases.

**Test File Requirements:**
- Create `Service/tests/test_evaluate_lesson_answer.py` with unit tests for prompt building, answer validation, lesson retrieval, and OpenAI integration
- Test cases: valid answers, empty answers, missing lessons, lessons without primary interactions, OpenAI API errors
- Mock OpenAIDAO responses for consistent testing

**Existing Test Modifications:**
- No modifications needed to existing tests
- New endpoint will be tested independently

**Validation Strategies:**
- Manual testing with actual lesson data from programs.jsonl
- Integration testing with OpenAI API (using test API keys)
- Response format validation against Pydantic models

## [Implementation Order]

Follow a sequential approach to minimize integration conflicts and ensure proper testing.

1. **Create Core Models and Endpoint File** - Define EvaluateAnswerRequest, EvaluateAnswerResponse, and basic endpoint structure in evaluate_lesson_answer.py

2. **Implement Prompt Building Logic** - Create build_evaluation_prompt function with comprehensive context integration including lesson conversation and primary interaction details

3. **Implement Main Evaluation Logic** - Complete evaluate_lesson_answer endpoint with lesson retrieval, validation, OpenAI integration, and response formatting

4. **Add Router Integration** - Update programs router.py to import and include the new evaluation endpoint

5. **Test Integration** - Verify endpoint registration, test with existing lesson data, validate response formats

6. **Create Comprehensive Tests** - Develop unit tests covering all functionality, edge cases, and error conditions

7. **Documentation and Validation** - Test complete workflow from lesson retrieval through OpenAI evaluation to response formatting
